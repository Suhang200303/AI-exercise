digraph {
	graph [size="53.25,53.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2406688253168 [label="
 (1, 100)" fillcolor=darkolivegreen1]
	2406761548768 [label=AddmmBackward0]
	2406761548864 -> 2406761548768
	2406688254448 [label="fc.bias
 (100)" fillcolor=lightblue]
	2406688254448 -> 2406761548864
	2406761548864 [label=AccumulateGrad]
	2406761548816 -> 2406761548768
	2406761548816 [label=ViewBackward0]
	2406761548624 -> 2406761548816
	2406761548624 [label=MeanBackward1]
	2406761549008 -> 2406761548624
	2406761549008 [label=ReluBackward0]
	2406761549104 -> 2406761549008
	2406761549104 [label=AddBackward0]
	2406761549200 -> 2406761549104
	2406761549200 [label=MulBackward0]
	2406761549296 -> 2406761549200
	2406761549296 [label=NativeBatchNormBackward0]
	2406761549488 -> 2406761549296
	2406761549488 [label=ConvolutionBackward0]
	2406761549680 -> 2406761549488
	2406761549680 [label=ReluBackward0]
	2406761547808 -> 2406761549680
	2406761547808 [label=NativeBatchNormBackward0]
	2406761548144 -> 2406761547808
	2406761548144 [label=ConvolutionBackward0]
	2406761799888 -> 2406761548144
	2406761799888 [label=ReluBackward0]
	2406761800032 -> 2406761799888
	2406761800032 [label=AddBackward0]
	2406761800128 -> 2406761800032
	2406761800128 [label=MulBackward0]
	2406761800272 -> 2406761800128
	2406761800272 [label=NativeBatchNormBackward0]
	2406761800416 -> 2406761800272
	2406761800416 [label=ConvolutionBackward0]
	2406761800608 -> 2406761800416
	2406761800608 [label=ReluBackward0]
	2406761800752 -> 2406761800608
	2406761800752 [label=NativeBatchNormBackward0]
	2406761800848 -> 2406761800752
	2406761800848 [label=ConvolutionBackward0]
	2406761801040 -> 2406761800848
	2406761801040 [label=ReluBackward0]
	2406761801184 -> 2406761801040
	2406761801184 [label=AddBackward0]
	2406761801280 -> 2406761801184
	2406761801280 [label=MulBackward0]
	2406761801424 -> 2406761801280
	2406761801424 [label=NativeBatchNormBackward0]
	2406761801568 -> 2406761801424
	2406761801568 [label=ConvolutionBackward0]
	2406761801760 -> 2406761801568
	2406761801760 [label=ReluBackward0]
	2406761801904 -> 2406761801760
	2406761801904 [label=NativeBatchNormBackward0]
	2406761802000 -> 2406761801904
	2406761802000 [label=ConvolutionBackward0]
	2406761802192 -> 2406761802000
	2406761802192 [label=ReluBackward0]
	2406761802336 -> 2406761802192
	2406761802336 [label=AddBackward0]
	2406761802432 -> 2406761802336
	2406761802432 [label=MulBackward0]
	2406761802720 -> 2406761802432
	2406761802720 [label=NativeBatchNormBackward0]
	2406761802960 -> 2406761802720
	2406761802960 [label=ConvolutionBackward0]
	2406761803152 -> 2406761802960
	2406761803152 [label=ReluBackward0]
	2406761803296 -> 2406761803152
	2406761803296 [label=NativeBatchNormBackward0]
	2406761803392 -> 2406761803296
	2406761803392 [label=ConvolutionBackward0]
	2406761802384 -> 2406761803392
	2406761802384 [label=ReluBackward0]
	2406761803680 -> 2406761802384
	2406761803680 [label=NativeBatchNormBackward0]
	2406761803728 -> 2406761803680
	2406761803728 [label=ConvolutionBackward0]
	2406761824512 -> 2406761803728
	2406688258944 [label="conv1.weight
 (32, 3, 3, 3)" fillcolor=lightblue]
	2406688258944 -> 2406761824512
	2406761824512 [label=AccumulateGrad]
	2406761803488 -> 2406761803680
	2406688258784 [label="bn1.weight
 (32)" fillcolor=lightblue]
	2406688258784 -> 2406761803488
	2406761803488 [label=AccumulateGrad]
	2406761824320 -> 2406761803680
	2406688258624 [label="bn1.bias
 (32)" fillcolor=lightblue]
	2406688258624 -> 2406761824320
	2406761824320 [label=AccumulateGrad]
	2406761803584 -> 2406761803392
	2406688257664 [label="layer1.0.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	2406688257664 -> 2406761803584
	2406761803584 [label=AccumulateGrad]
	2406761803344 -> 2406761803296
	2406688257824 [label="layer1.0.bn1.weight
 (32)" fillcolor=lightblue]
	2406688257824 -> 2406761803344
	2406761803344 [label=AccumulateGrad]
	2406761803200 -> 2406761803296
	2406688257504 [label="layer1.0.bn1.bias
 (32)" fillcolor=lightblue]
	2406688257504 -> 2406761803200
	2406761803200 [label=AccumulateGrad]
	2406761803104 -> 2406761802960
	2406688256544 [label="layer1.0.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	2406688256544 -> 2406761803104
	2406761803104 [label=AccumulateGrad]
	2406761802912 -> 2406761802720
	2406688256464 [label="layer1.0.bn2.weight
 (32)" fillcolor=lightblue]
	2406688256464 -> 2406761802912
	2406761802912 [label=AccumulateGrad]
	2406761802864 -> 2406761802720
	2406688257344 [label="layer1.0.bn2.bias
 (32)" fillcolor=lightblue]
	2406688257344 -> 2406761802864
	2406761802864 [label=AccumulateGrad]
	2406761802528 -> 2406761802432
	2406761802528 [label=ViewBackward0]
	2406761803536 -> 2406761802528
	2406761803536 [label=SigmoidBackward0]
	2406761803248 -> 2406761803536
	2406761803248 [label=MmBackward0]
	2406761803632 -> 2406761803248
	2406761803632 [label=ReluBackward0]
	2406761824608 -> 2406761803632
	2406761824608 [label=MmBackward0]
	2406761824704 -> 2406761824608
	2406761824704 [label=ViewBackward0]
	2406761824848 -> 2406761824704
	2406761824848 [label=MeanBackward1]
	2406761802720 -> 2406761824848
	2406761824560 -> 2406761824608
	2406761824560 [label=TBackward0]
	2406761824896 -> 2406761824560
	2406745798272 [label="layer1.0.se.fc1.weight
 (2, 32)" fillcolor=lightblue]
	2406745798272 -> 2406761824896
	2406761824896 [label=AccumulateGrad]
	2406761803008 -> 2406761803248
	2406761803008 [label=TBackward0]
	2406761824944 -> 2406761803008
	2406745526576 [label="layer1.0.se.fc2.weight
 (32, 2)" fillcolor=lightblue]
	2406745526576 -> 2406761824944
	2406761824944 [label=AccumulateGrad]
	2406761802384 -> 2406761802336
	2406761802144 -> 2406761802000
	2406688221680 [label="layer2.0.conv1.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	2406688221680 -> 2406761802144
	2406761802144 [label=AccumulateGrad]
	2406761801952 -> 2406761801904
	2406688221440 [label="layer2.0.bn1.weight
 (64)" fillcolor=lightblue]
	2406688221440 -> 2406761801952
	2406761801952 [label=AccumulateGrad]
	2406761801808 -> 2406761801904
	2406688221520 [label="layer2.0.bn1.bias
 (64)" fillcolor=lightblue]
	2406688221520 -> 2406761801808
	2406761801808 [label=AccumulateGrad]
	2406761801712 -> 2406761801568
	2406688213888 [label="layer2.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2406688213888 -> 2406761801712
	2406761801712 [label=AccumulateGrad]
	2406761801520 -> 2406761801424
	2406688213808 [label="layer2.0.bn2.weight
 (64)" fillcolor=lightblue]
	2406688213808 -> 2406761801520
	2406761801520 [label=AccumulateGrad]
	2406761801472 -> 2406761801424
	2406688213648 [label="layer2.0.bn2.bias
 (64)" fillcolor=lightblue]
	2406688213648 -> 2406761801472
	2406761801472 [label=AccumulateGrad]
	2406761801376 -> 2406761801280
	2406761801376 [label=ViewBackward0]
	2406761802096 -> 2406761801376
	2406761802096 [label=SigmoidBackward0]
	2406761801856 -> 2406761802096
	2406761801856 [label=MmBackward0]
	2406761802240 -> 2406761801856
	2406761802240 [label=ReluBackward0]
	2406761803440 -> 2406761802240
	2406761803440 [label=MmBackward0]
	2406761802288 -> 2406761803440
	2406761802288 [label=ViewBackward0]
	2406761824656 -> 2406761802288
	2406761824656 [label=MeanBackward1]
	2406761801424 -> 2406761824656
	2406761824368 -> 2406761803440
	2406761824368 [label=TBackward0]
	2406761824992 -> 2406761824368
	2406688213168 [label="layer2.0.se.fc1.weight
 (4, 64)" fillcolor=lightblue]
	2406688213168 -> 2406761824992
	2406761824992 [label=AccumulateGrad]
	2406761802480 -> 2406761801856
	2406761802480 [label=TBackward0]
	2406761803056 -> 2406761802480
	2406688213088 [label="layer2.0.se.fc2.weight
 (64, 4)" fillcolor=lightblue]
	2406688213088 -> 2406761803056
	2406761803056 [label=AccumulateGrad]
	2406761801232 -> 2406761801184
	2406761801232 [label=NativeBatchNormBackward0]
	2406761802048 -> 2406761801232
	2406761802048 [label=ConvolutionBackward0]
	2406761802192 -> 2406761802048
	2406761825184 -> 2406761802048
	2406744896512 [label="layer2.0.downsample.0.weight
 (64, 32, 1, 1)" fillcolor=lightblue]
	2406744896512 -> 2406761825184
	2406761825184 [label=AccumulateGrad]
	2406761801664 -> 2406761801232
	2406688222800 [label="layer2.0.downsample.1.weight
 (64)" fillcolor=lightblue]
	2406688222800 -> 2406761801664
	2406761801664 [label=AccumulateGrad]
	2406761801328 -> 2406761801232
	2406688222640 [label="layer2.0.downsample.1.bias
 (64)" fillcolor=lightblue]
	2406688222640 -> 2406761801328
	2406761801328 [label=AccumulateGrad]
	2406761800992 -> 2406761800848
	2406688211968 [label="layer3.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2406688211968 -> 2406761800992
	2406761800992 [label=AccumulateGrad]
	2406761800800 -> 2406761800752
	2406688211888 [label="layer3.0.bn1.weight
 (128)" fillcolor=lightblue]
	2406688211888 -> 2406761800800
	2406761800800 [label=AccumulateGrad]
	2406761800656 -> 2406761800752
	2406688211728 [label="layer3.0.bn1.bias
 (128)" fillcolor=lightblue]
	2406688211728 -> 2406761800656
	2406761800656 [label=AccumulateGrad]
	2406761800560 -> 2406761800416
	2406688211168 [label="layer3.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2406688211168 -> 2406761800560
	2406761800560 [label=AccumulateGrad]
	2406761800368 -> 2406761800272
	2406688211008 [label="layer3.0.bn2.weight
 (128)" fillcolor=lightblue]
	2406688211008 -> 2406761800368
	2406761800368 [label=AccumulateGrad]
	2406761800320 -> 2406761800272
	2406688214368 [label="layer3.0.bn2.bias
 (128)" fillcolor=lightblue]
	2406688214368 -> 2406761800320
	2406761800320 [label=AccumulateGrad]
	2406761800224 -> 2406761800128
	2406761800224 [label=ViewBackward0]
	2406761800944 -> 2406761800224
	2406761800944 [label=SigmoidBackward0]
	2406761800704 -> 2406761800944
	2406761800704 [label=MmBackward0]
	2406761801088 -> 2406761800704
	2406761801088 [label=ReluBackward0]
	2406761801136 -> 2406761801088
	2406761801136 [label=MmBackward0]
	2406761825136 -> 2406761801136
	2406761825136 [label=ViewBackward0]
	2406761825328 -> 2406761825136
	2406761825328 [label=MeanBackward1]
	2406761800272 -> 2406761825328
	2406761824464 -> 2406761801136
	2406761824464 [label=TBackward0]
	2406761825376 -> 2406761824464
	2406688276864 [label="layer3.0.se.fc1.weight
 (8, 128)" fillcolor=lightblue]
	2406688276864 -> 2406761825376
	2406761825376 [label=AccumulateGrad]
	2406761801616 -> 2406761800704
	2406761801616 [label=TBackward0]
	2406761825424 -> 2406761801616
	2406688265856 [label="layer3.0.se.fc2.weight
 (128, 8)" fillcolor=lightblue]
	2406688265856 -> 2406761825424
	2406761825424 [label=AccumulateGrad]
	2406761800080 -> 2406761800032
	2406761800080 [label=NativeBatchNormBackward0]
	2406761800896 -> 2406761800080
	2406761800896 [label=ConvolutionBackward0]
	2406761801040 -> 2406761800896
	2406761825472 -> 2406761800896
	2406688212848 [label="layer3.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2406688212848 -> 2406761825472
	2406761825472 [label=AccumulateGrad]
	2406761800512 -> 2406761800080
	2406688212448 [label="layer3.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2406688212448 -> 2406761800512
	2406761800512 [label=AccumulateGrad]
	2406761800176 -> 2406761800080
	2406688212368 [label="layer3.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2406688212368 -> 2406761800176
	2406761800176 [label=AccumulateGrad]
	2406761799840 -> 2406761548144
	2406688264416 [label="layer4.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2406688264416 -> 2406761799840
	2406761799840 [label=AccumulateGrad]
	2406761548096 -> 2406761547808
	2406688264256 [label="layer4.0.bn1.weight
 (256)" fillcolor=lightblue]
	2406688264256 -> 2406761548096
	2406761548096 [label=AccumulateGrad]
	2406761549728 -> 2406761547808
	2406688268176 [label="layer4.0.bn1.bias
 (256)" fillcolor=lightblue]
	2406688268176 -> 2406761549728
	2406761549728 [label=AccumulateGrad]
	2406761549632 -> 2406761549488
	2406688267456 [label="layer4.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2406688267456 -> 2406761549632
	2406761549632 [label=AccumulateGrad]
	2406761549440 -> 2406761549296
	2406688266016 [label="layer4.0.bn2.weight
 (256)" fillcolor=lightblue]
	2406688266016 -> 2406761549440
	2406761549440 [label=AccumulateGrad]
	2406761549392 -> 2406761549296
	2406688255408 [label="layer4.0.bn2.bias
 (256)" fillcolor=lightblue]
	2406688255408 -> 2406761549392
	2406761549392 [label=AccumulateGrad]
	2406761549344 -> 2406761549200
	2406761549344 [label=ViewBackward0]
	2406761549776 -> 2406761549344
	2406761549776 [label=SigmoidBackward0]
	2406761549536 -> 2406761549776
	2406761549536 [label=MmBackward0]
	2406761799936 -> 2406761549536
	2406761799936 [label=ReluBackward0]
	2406761799984 -> 2406761799936
	2406761799984 [label=MmBackward0]
	2406761825040 -> 2406761799984
	2406761825040 [label=ViewBackward0]
	2406761825616 -> 2406761825040
	2406761825616 [label=MeanBackward1]
	2406761549296 -> 2406761825616
	2406761825088 -> 2406761799984
	2406761825088 [label=TBackward0]
	2406761825664 -> 2406761825088
	2406688254928 [label="layer4.0.se.fc1.weight
 (16, 256)" fillcolor=lightblue]
	2406688254928 -> 2406761825664
	2406761825664 [label=AccumulateGrad]
	2406761800464 -> 2406761549536
	2406761800464 [label=TBackward0]
	2406761825712 -> 2406761800464
	2406688254768 [label="layer4.0.se.fc2.weight
 (256, 16)" fillcolor=lightblue]
	2406688254768 -> 2406761825712
	2406761825712 [label=AccumulateGrad]
	2406761549152 -> 2406761549104
	2406761549152 [label=NativeBatchNormBackward0]
	2406761549584 -> 2406761549152
	2406761549584 [label=ConvolutionBackward0]
	2406761799888 -> 2406761549584
	2406761825760 -> 2406761549584
	2406688265536 [label="layer4.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2406688265536 -> 2406761825760
	2406761825760 [label=AccumulateGrad]
	2406761549248 -> 2406761549152
	2406688265376 [label="layer4.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2406688265376 -> 2406761549248
	2406761549248 [label=AccumulateGrad]
	2406761799744 -> 2406761549152
	2406688265216 [label="layer4.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2406688265216 -> 2406761799744
	2406761799744 [label=AccumulateGrad]
	2406761548720 -> 2406761548768
	2406761548720 [label=TBackward0]
	2406761549056 -> 2406761548720
	2406688254608 [label="fc.weight
 (100, 256)" fillcolor=lightblue]
	2406688254608 -> 2406761549056
	2406761549056 [label=AccumulateGrad]
	2406761548768 -> 2406688253168
}
